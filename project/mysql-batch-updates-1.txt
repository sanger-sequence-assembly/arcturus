Date: Fri, 23 May 2008 14:30:36 +0100
From: David Harper <adh@sanger.ac.uk>
To: MySQL mailing list <mysql@sanger.ac.uk>
Subject: Batch updates in Perl and Java

Hello folks,

I thought you might be interested in the results of a benchmarking test that I've been running this morning.  I wanted to determine whether there was any performance gain in using batch updates to insert data in bulk into a MySQL table rather than a series of single-row inserts.

*** If you don't do MySQL client programming in either Perl or Java, ***
*** this would be a good point at which to hit the delete button :-) ***

Both Perl and Java provide methods for batch updates through prepared statement objects: in Perl, the bind_param_array and execute_array methods, and in Java, the addBatch and executeBatch methods.

I ran some tests using the attached Perl script and Java class which insert data into the following table:

CREATE TABLE `SEQUENCE` (
  `seq_id` mediumint(8) unsigned NOT NULL default '0',
  `seqlen` int(11) NOT NULL,
  `seq_hash` binary(16) default NULL,
  `qual_hash` binary(16) default NULL,
  `sequence` mediumblob NOT NULL,
  `quality` mediumblob NOT NULL,
  PRIMARY KEY  (`seq_id`)
)

My test runs inserted 100,000 rows in which the 'sequence' and 'quality' columns were random binary data of 500 ± 100 bytes and 600 ± 100 bytes respectively, to simulate compressed data of varying sizes.  Each test run uploaded around 120 megabytes of data to the server.

I ran a test in which each row was inserted separately, and then three tests in which the rows were batched into sets of 1000, 5000 and 10,000 respectively.

I used MyISAM and InnoDB versions of the table on the same MySQL instance, and out of curiosity, I repeated the tests on a PostgreSQL instance running on the same machine.

Both MySQL and PostgreSQL were very recent stable releases (5.0.51a and 8.3.1 respectively) installed "out of the box" without any tuning.

Here are the results, expressed as wall-clock time in seconds reported by "time" command:

Perl DBI:
                No batch    1000 rows      5000 rows      10000 rows
----------------------------------------------------------------------
PostgreSQL         453         399            395            397

MyISAM              46          45             45             45

InnoDB             365         368            333            328

Java JDBC:
                No batch    1000 rows      5000 rows      10000 rows
----------------------------------------------------------------------
PostgreSQL         332          16             15             18

MyISAM              29          32             29             29

InnoDB             267         263            262            267

Neither the MySQL DBI driver nor the MySQL JDBC driver achieves any performance gain through using batched inserts.  I'm guessing that neither driver converts them to multi-row insert statements, which are known to be faster than "one row at a time".

The PostgreSQL JDBC driver achieves a dramatic gain in performance when batched inserts are used, speeding up the process by a factor of 20. The DBI driver gives a more modest gain.

The DBI bind_param_array method binds an array of values to a specific *column* of the table, so you have to construct a set of arrays, one per column, with as many entries as rows that you want to insert.  In my test program, for example, I'm inserting into 6 columns, so I have to build 6 arrays, each with 1000, 5000 or 10,000 entries depending on the batch size.  This is rather counter-intuitive, and leads to more complex code than the Java version.

Feel free to share this with anyone who may be interested.

Cheers

David 
